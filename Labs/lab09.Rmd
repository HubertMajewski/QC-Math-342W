---
title: "Lab 9"
author: "Hubert Majewski"
output: pdf_document
date: "11:59PM May 10, 2021"
---

Here we will learn about trees, bagged trees and random forests. You can use the `YARF` package if it works, otherwise, use the `randomForest` package (the standard).

Let's take a look at the simulated sine curve data from practice lecture 12. Below is the code for the data generating process:

```{r}

#Turn off warnings
options(warn = -1)

rm(list = ls())
n = 500
sigma = 0.3
x_min = 0
x_max = 10
f_x = function(x){sin(x)}
y_x = function(x, sigma){f_x(x) + rnorm(n, 0, sigma)}
x_train = runif(n, x_min, x_max)
y_train = y_x(x_train, sigma)
```

Plot an example dataset of size 500:

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(x=x_train, y=y_train))+
	geom_point(aes(x=x, y=y))
```

Create a test set of size 500 as well

```{r}

x_test = runif(n, x_min, x_max)
y_test = y_x(x_test, sigma)

```

Locate the optimal node size hyperparameter for the regression tree model. I believe you can use `randomForest` here by setting `ntree = 1`, `replace = FALSE`, `sampsize = n` (`mtry` is already set to be 1 because there is only one feature) and then you can set `nodesize`. Plot nodesize by oos_se.

```{r}

pacman::p_load(randomForest)

nodesize = 1:n

se_by_nodes <- array(NA, length(nodesize))

for(i in 1:length(nodesize)) {

	rf_mod <- randomForest(x = data.frame(x = x_train), y= y_train, ntree = 1, replace = FALSE, sampsize = n, nodesize = nodesize[i])
	
	y_hat_test <- predict(rf_mod, data.frame(x= x_test))
		
	se_by_nodes[i] <- sd(y_test - y_hat_test)

}

ggplot(data.frame(x=nodesize, y=se_by_nodes)) + 
	geom_line(aes(x=x, y=y)) + 
	scale_x_reverse()

which.min(se_by_nodes)
```

Plot the regression tree model with the optimal node size.

```{r}

rf_mod <- randomForest(x = data.frame(x = x_train), y=y_train, ntree = 1, replace = FALSE, sampsize = n, nodesize = nodesize[which.min(se_by_nodes)])

res <- 0.01

x_grid = seq(from=x_min, to=x_max, by = res)

g_x <- predict(rf_mod, data.frame(x= x_grid))

ggplot(data.frame(x=x_grid, y=g_x)) + 
	aes(x=x, y=y) +
	geom_point(data=data.frame(x=x_train, y=y_train)) + 
	geom_point(aes(x=x, y=y), color="blue")

```

Provide the bias-variance decomposition of this DGP fit with this model. It is a lot of code, but it is in the practice lectures. If your three numbers don't add up within two significant digits, increase your resolution.

```{r}

ntrain <- 25
ntest <- 900
nsim <- 1000

train <- matrix(NA, nsim, 2)
xtrains <- matrix(NA, nsim, ntrain)
ytrains <- matrix(NA, nsim, ntrain)
oos_res <- matrix(NA, nsim, ntest)

for(sim in 1:nsim) {
		
	xtrain <- runif(ntrain,x_min, x_max)
	deltaTrain <- rnorm(ntrain, 0, sigma)
	ytrain <- f_x(xtrain) + deltaTrain
	xtrains[sim, ] <- xtrain
	ytrains[sim, ] <- ytrain
	mod <- lm(ytrain ~., data.frame(x = xtrain))
	train[sim, ] = coef(mod)
	xtest <- runif(ntest, x_min, x_max)
	deltaTest <- rnorm(ntest, 0, sigma)
	ytest <- f_x(xtest) + deltaTest
	yHatTest <- predict(mod, data.frame(x = xtest))
	oos_res[sim, ] <- ytest - yHatTest
	
}

#Plotting
pacman::p_load(ggplot2)
res <- 15675
x <- seq(x_min, x_max, length.out = res)
lavg <- colMeans(train)
f_x_df <- data.frame(x = x, z = f_x(x))

ggplot(f_x_df, aes(x, z, p)) + 
	geom_line(col = "blue") + 
	geom_point(aes(x, y), data=data.frame(x = xtrains[1,], y = ytrains[1,])) + 
	geom_abline(intercept = lavg[1], slope = lavg[2], col = "green", lwd = 1.7)


#Concat all lines
plots <- ggplot() + 
	xlim(x_min, x_max) + 
	ylim(x_min ^ 2, x_max ^ 2)

for (sim in 1:min(nsim, 175)) {
		
	plots <- plots + geom_abline(slope = train[sim, 2], intercept = train[sim, 1], col = "black", lwd = .75)
	
}

plots +
	geom_abline(slope = lavg[2], intercept = lavg[1], col = "blue", lwd = 1.7) +
	ylim(-1.5, 1.5)

#Mse
mse <- mean(c(oos_res) ^ 2)
mse

#Sigma squared
sigma ^ 2

#Average variance
varArray <- array(NA, nsim)
for(sim in 1:nsim) {
	
	varArray[sim] <- mean(( train[sim, 1] + train[sim, 2] * seq(x_min, x_max, length.out = res)
										 - lavg[1] - lavg[2] * seq(x_min, x_max, length.out = res)
									) ^ 2)
	
}
mean(varArray)

#Bias
funcT <- sin(x)
bias <- funcT - lavg[1] - lavg[2] * seq(x_min, x_max, length.out = res)
mean(bias ^ 2)

#Sums
sigma ^ 2 + mean(varArray) + mean(bias ^ 2)
```


```{r}
rm(list = ls())
```

Take a sample of n = 2000 observations from the diamonds data.

```{r}
pacman::p_load(dplyr)
diamonds_samp <- diamonds %>%
	sample_n(2000)
```

find the bootstrap s_e for a RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can calculate oob residuals via `e_oob = y_train - rf_mod$predicted`.

```{r}
num_trees = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_num_trees <- array(NA, length(num_trees))

for(i in 1:length(num_trees)) {
	
	rf_mod <- randomForest(price~., data=diamonds_samp, ntree=num_trees[i])
	
	oob_se_num_trees[i] <- sd(diamonds_samp$price - rf_mod$predicted)
}

ggplot(data.frame(x=num_trees, y=oob_se_num_trees)) + 
	geom_line(aes(x=x,y=y))


```

Using the diamonds data, find the bootstrap s_e for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can create the bagged tree model via setting an argument within the RF constructor function. 

```{r}

num_trees = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_num_trees_bag <- array(NA, length(num_trees))

for(i in 1:length(num_trees)) {
	
	rf_mod <- randomForest(price~., data=diamonds_samp, ntree=num_trees[i], mtry = ncol(diamonds_samp) - 1)
	
	oob_se_num_trees_bag[i] <- sd(diamonds_samp$price - rf_mod$predicted)
}

ggplot(data.frame(x=num_trees, y=oob_se_num_trees_bag)) + 
	geom_line(aes(x=x,y=y))

```


What is the percentage gain / loss in performance of the RF model vs bagged trees model?

```{r}
(oob_se_num_trees - oob_se_num_trees_bag) / oob_se_num_trees_bag * 100
```


Plot bootstrap s_e by number of trees for both RF and bagged trees.

```{r}

ggplot(
	rbind(
		data.frame(num_trees=num_trees, value=oob_se_num_trees, model="RF"), 
		data.frame(num_trees=num_trees, value=oob_se_num_trees_bag, model="BAG")
	)
) + geom_line(aes(x=num_trees, y=value, color = model))


```

Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum. That maximum will be the number of features assuming that we do not binarize categorical features if you are using `randomForest` or the number of features assuming binarization of the categorical features if you are using `YARF`. Calculate bootstrap s_e for all mtry values.

```{r}

mtrys = 1:(ncol(diamonds_samp) - 1)
oob_se_num_mtry <- array(NA, length(mtrys))

for(i in 1:length(mtrys)) {
	
	rf_mod <- randomForest(price~., data=diamonds_samp, mtry = mtrys[i])
	
	oob_se_num_mtry[i] <- sd(diamonds_samp$price - rf_mod$predicted)
	
}

ggplot(data.frame(x=mtrys, y=oob_se_num_mtry)) + 
	geom_line(aes(x=x,y=y))


```


```{r}
rm(list = ls())
```


Take a sample of n = 2000 observations from the adult data.

```{r}

pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult)

adult_samp = adult %>%
	sample_n(2000)
```

Using the adult data, find the bootstrap misclassification error for an RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. Plot it aswel.

```{r}

num_trees = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_num_trees <- array(NA, length(num_trees))

for(i in 1:length(num_trees)) {
	
	rf_mod <- randomForest(income~., data=adult_samp, ntree=num_trees[i])
	
	oob_se_num_trees[i] <- mean(adult_samp$income != rf_mod$predicted ) #sd(adult$income - rf_mod$predicted)
}

ggplot(data.frame(x=num_trees, y=oob_se_num_trees)) + 
	geom_line(aes(x=x,y=y))

```

Using the adult data, find the bootstrap misclassification error for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees.

```{r}
oob_se_num_trees_bag <- array(NA, length(num_trees))

for(i in 1:length(num_trees)) {
	
	rf_mod <- randomForest(income~., data=adult_samp, ntree=num_trees[i], mtry = ncol(adult) - 1)
	
	oob_se_num_trees_bag[i] <- mean(adult_samp$income != rf_mod$predicted) #sd(adult$income - rf_mod$predicted)
}

ggplot(data.frame(x=num_trees, y=oob_se_num_trees_bag)) + 
	geom_line(aes(x=x,y=y))

```


What is the percentage gain / loss in performance of the RF model vs bagged trees model?

```{r}

tmp <- oob_se_num_trees - oob_se_num_trees_bag
tmp <- tmp / oob_se_num_trees_bag * 100

tmp
```


Plot bootstrap misclassification error by number of trees for both RF and bagged trees.

```{r}

ggplot(
	rbind(
		data.frame(num_trees=num_trees, value=oob_se_num_trees, model="RF"), 
		data.frame(num_trees=num_trees, value=oob_se_num_trees_bag, model="BAG")
	)
) + geom_line(aes(x=num_trees, y=value, color = model))

```

Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum (see above as maximum is defined by the specific RF algorithm implementation).

```{r}

mtry <- 1:(ncol(adult_samp) - 1)

oob <- array(NA, length(mtry))

for(i in 1:length(mtry)) {
	
	mod <- randomForest(income ~., data = adult_samp, mtry = mtry[i])
	
	oob[i] <- mean(adult_samp$income != mod$predicted)
	
}

```

Plot bootstrap misclassification error by `mtry`.

```{r}
ggplot(data.frame(x=mtry, y=oob)) + 
	geom_line(aes(x=x,y=y))
```


```{r}
rm(list = ls())
```

Write a function `random_bagged_ols` which takes as its arguments `X` and `y` with further arguments `num_ols_models` defaulted to 100 and `mtry` defaulted to NULL which then gets set within the function to be 50% of available features. This argument builds an OLS on a bootstrap sample of the data and uses only `mtry < p` of the available features. The function then returns all the `lm` models as a list with size `num_ols_models`.

```{r}

random_bagged_ols <- function(x, y, num_ols_models = 100, mtry = NULL) {
	
	#Default values
	if (is.null(mtry) | mtry >= ncol(X))
		mtry <- .5 * ncol(X)
	
	#Models
	lm_mod <- array(NA, num_ols_models)
	
	nTrain = .1 * nrow(X)
	
	#Execute
	for(i in 1:num_ols_models) {
		
		bootstrapI <- sample(1:nTrain, replace = TRUE)
		select <- sample(1:mtr, replace = TRUE)
		
		lm_mod[i] <- lm(y[bootstrapI] ~ x[, select], x[bootstrapI])
		
	}
	
	return(lm_mod)
}

```


Load up the Boston Housing Data and separate into `X` and `y`.

```{r}

pacman::p_load(MASS)

data(Boston)

y <- Boston$medv
x <- Boston - y

x$mdev <- NULL;

```

Similar to lab 1, write a function that takes a matrix and punches holes (i.e. sets entries equal to `NA`) randomly with an argument `prob_missing`.

```{r}

punch <- function(x, prob_missing) {
	
	nr <- nrow(x) 
	nc <- ncol(x)
	
	m <- matrix(rbinom(nr * nc, 1, prob_missing), nrow = nr, ncol = nc)
	
  x[m == 1] = NA
  
  return(x)
	
}

```

Create a matrix `Xmiss` which is `X` but has missingness with probability of 10%.

```{r}

xmiss <- punch(x, .1)

```

Use a random forest modeling procedure to iteratively fill in the `NA`'s by predicting each feature of X using every other feature of X. You need to start by filling in the holes to use RF. So fill them in with the average of the feature.

```{r}
pacman::p_load(randomForest, tidyr)

#Turn off warnings
options(warn = -1)

x <- data.frame(xmiss)
nr <- nrow(x)
nc <- ncol(x)
P <- data.frame(matrix(NA, nr, nc))

for(i in 1:nr) {
	
  for(j in 1:nc) {
  	
    if(is.na(x[i, j])) {
    	
      naive <- x %>% 
      	replace_na(
      		as.list(
      			colMeans(x, na.rm = TRUE)
      		)
      	)
      
      mod <- randomForest(naive[ , j] ~ ., 
															data = naive, ntree = 100)
      
      #Store prediction
      P[i,j] <- predict(mod, naive[i, ])
      x[i,j] <- predict(mod, naive[i, ])
      
    }
  	
  }
	
}

#Averaged predictions
x

#Debugging
#P

```






